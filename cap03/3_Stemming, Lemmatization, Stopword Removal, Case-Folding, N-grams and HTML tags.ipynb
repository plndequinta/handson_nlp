{"cells":[{"cell_type":"markdown","metadata":{"id":"GX5yPvpVhg4s"},"source":["# Understanding word normalization\n","@author: Aman Kedia"]},{"cell_type":"markdown","metadata":{"id":"01SLVs96hg40"},"source":["Na maioria das vezes, não queremos ter cada fragmento de palavra individual que já encontramos em nosso vocabulário. Poderíamos desejar isso por várias razões, uma delas é a necessidade de distinguir corretamente (por exemplo) a frase U.N. (com caracteres separados por um ponto) de UN (sem pontos). Também podemos trazer palavras à sua forma raiz no dicionário. Por exemplo, am, are e is podem ser identificados por sua forma raiz, be. Em outra frente, podemos remover as inflexões das palavras para trazê-las à mesma forma.\n","\n","As palavras carro, carros e carros podem ser identificadas como carro.\n","\n","Além disso, palavras comuns que ocorrem com muita frequência e não transmitem muito significado, como os artigos a, an e the, podem ser removidas. \n","\n","No entanto, tudo isso depende muito dos casos de uso. Palavras como **(when, why, where, and who)Wh-words**, por exemplo, quando, por que, onde e quem, não carregam muitas informações na maioria dos contextos e são removidos como parte de uma técnica chamada **stopword removal** remoção de palavras irrelevantes, que veremos um pouco mais tarde na seção Remoção de palavras irrelevantes; no entanto, em situações como classificação e resposta de perguntas, essas palavras tornam-se muito importantes e não devem ser removidas. Agora, com uma compreensão básica dessas técnicas, vamos nos aprofundar nelas em detalhe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y43uGoQ3hg44"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"WfmcQAh4hg49"},"source":["### Stemming\n","\n","Imagine reunir todas as palavras **computador, computadorização e computadorizar** em uma palavra, computar. O que acontece aqui é chamado de **stemming** derivação. Como parte da stemming, uma tentativa grosseira é feita para remover as formas flexionais de uma palavra e trazê-las para uma forma básica chamada radical. As peças cortadas são chamadas de afixos. No exemplo anterior, compute é a forma básica e os afixos são r, rization e rize, respectivamente. Uma coisa a ter em mente é que o radical não precisa ser uma palavra válida como a conhecemos. Por exemplo, o\n","a palavra tradicional seria derivada de tradit, que não é uma palavra válida no dicionário inglês.\n","\n","Os dois algoritmos / métodos mais comuns empregados para o stemming  incluem o Porter stemmer  e o Snowball stemmer. O  Porter oferece suporte para o idioma inglês, enquanto o  Snowball, que é um aprimoramento do  Porter, oferece suporte a vários idiomas, o que pode ser visto no seguinte trecho de código e sua saída:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQoKRzuzhg4-"},"outputs":[],"source":["import nltk"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0-F-u94Ghg5C","executionInfo":{"status":"ok","timestamp":1639697145862,"user_tz":180,"elapsed":2544,"user":{"displayName":"Luiz Fernando","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzVXVMEu4cduyxAHXqlAme_nJ2Hq7JuM4l1TkL-70=s64","userId":"10907072071610206186"}},"outputId":"fc423626-2a40-4c04-e6f9-baecf89e437c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"]}],"source":["from nltk.stem.snowball import SnowballStemmer\n","print(SnowballStemmer.languages)"]},{"cell_type":"markdown","metadata":{"id":"BiuUftYjhg5G"},"source":["Uma coisa a se notar no trecho é que o Porter é uma das opçoes providas pela Snowball.Outros stemmers incluem Lancaster, Dawson, Krovetz e lematizadores Lovins, entre outros. Veremos os stemmer Porter e Snowball em detalhes aqui. O stemmer Porter funciona apenas com strings, enquanto o stemmer Snowball funciona com strings e dados Unicode. O stemmer Snowball também permite a opção de ignorar palavras irrelevantes como uma funcionalidade inerente. Agora, vamos primeiro aplicar o stemmer Porter às palavras e ver seus efeitos no seguinte bloco de código:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"KJfErFPIhg5K","executionInfo":{"status":"ok","timestamp":1639697152511,"user_tz":180,"elapsed":4,"user":{"displayName":"Luiz Fernando","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzVXVMEu4cduyxAHXqlAme_nJ2Hq7JuM4l1TkL-70=s64","userId":"10907072071610206186"}}},"outputs":[],"source":["plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n","           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']"]},{"cell_type":"markdown","metadata":{"id":"EePmWhVPhg5M"},"source":["# Porter Stemmer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKK4h4-Phg5P","outputId":"89938615-2191-4ccf-a1e5-3f900fae7151"},"outputs":[{"name":"stdout","output_type":"stream","text":["caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"]}],"source":["from nltk.stem.porter import PorterStemmer \n","stemmer = PorterStemmer()\n","singles = [stemmer.stem(plural) for plural in plurals]\n","print(' '.join(singles))"]},{"cell_type":"markdown","metadata":{"id":"cBxccqqHhg5S"},"source":["# Snowball Stemmer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iuV1eThhg5v","outputId":"8adade75-9663-4071-b37c-1ab1ee34218d"},"outputs":[{"name":"stdout","output_type":"stream","text":["('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"]}],"source":["from nltk.stem.snowball import SnowballStemmer\n","print(SnowballStemmer.languages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPgPLu2chg50","outputId":"0d46e5a1-1488-4868-937f-b95f9ceac803"},"outputs":[{"name":"stdout","output_type":"stream","text":["caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"]}],"source":["stemmer2 = SnowballStemmer(language='english')\n","singles = [stemmer2.stem(plural) for plural in plurals]\n","print(' '.join(singles))"]},{"cell_type":"markdown","metadata":{"id":"rVYHt-jNhg52"},"source":["### Lemmatização\n","\n","Ao contrário da stemming, em que alguns caracteres são removidos das palavras usando métodos rudes, a lematização é um processo em que o contexto é usado para converter uma palavra em sua forma de base significativa. Ajuda a agrupar palavras que têm uma forma base comum e, portanto, podem ser identificadas como um único item. A forma de base é conhecida como lema da palavra e às vezes também é conhecida como forma de dicionário.\n","\n","Os algoritmos de lematização tentam identificar a forma do lemma de uma palavra levando em consideração o contexto da proximidade da palavra, as marcas da classe gramatical (**parto of speech**POS), o significado de uma palavra e assim por diante. A vizinhança pode abranger palavras na vizinhança, frases ou até mesmo documentos.\n","\n","Além disso, as mesmas palavras podem ter lemas diferentes dependendo do contexto. Um lematizador tentaria identificar as marcas de classe gramatical com base no contexto para identificar o lema apropriado. O lematizador mais comumente usado é o lematizador WordNet. Outros lematizadores incluem o  Spacy, o TextBlob e o  Gensim, entre outros. Nesta seção, exploraremos os lematizadores WordNet e Spacy.\n"]},{"cell_type":"markdown","metadata":{"id":"hXfD7v6_hg54"},"source":["# Wordnet Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJxugPMEhg54","outputId":"345724a3-b6c9-4c5b-a985-b79643c20d2c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/carlos/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFcZ2OgZhg55","outputId":"163b35e0-7c2b-46ee-e9e2-61cf1359013c"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n","The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"]}],"source":["lemmatizer = WordNetLemmatizer()\n","s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n","token_list = s.split()\n","print(\"The tokens are: \", token_list)\n","lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n","print(\"The lemmatized output is: \", lemmatized_output)"]},{"cell_type":"markdown","metadata":{"id":"MeERnqslhg58"},"source":["O lematizador WordNet funciona bem se as tags POS também forem fornecidas como entradas.\n","\n","É realmente impossível anotar manualmente cada palavra com sua tag POS em um corpus de texto.\n","Agora, como resolvemos esse problema e fornecemos as marcas de classe gramatical para palavras individuais como entrada para o lematizador do WordNet?\n","\n","Felizmente, a biblioteca nltk fornece um método para localizar tags POS para uma lista de palavras usando um tagger perceptron médio, **cujos detalhes estão fora do escopo deste capítulo.**\n","\n","As tags POS para a frase  **We are trying our best to understand Lemmatization** pode ser encontrada no seguinte trecho de código:"]},{"cell_type":"markdown","metadata":{"id":"r9VoquXphg59"},"source":["## POS Tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWlTroblhg59","outputId":"4f17f430-178d-43f2-96e8-14f06f42d21e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/carlos/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["[('We', 'PRP'),\n"," ('are', 'VBP'),\n"," ('putting', 'VBG'),\n"," ('in', 'IN'),\n"," ('efforts', 'NNS'),\n"," ('to', 'TO'),\n"," ('enhance', 'VB'),\n"," ('our', 'PRP$'),\n"," ('understanding', 'NN'),\n"," ('of', 'IN'),\n"," ('Lemmatization', 'NN')]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('averaged_perceptron_tagger')\n","pos_tags = nltk.pos_tag(token_list)\n","pos_tags"]},{"cell_type":"markdown","metadata":{"id":"7iZ0KEVohg5_"},"source":["## POS tag Mapping"]},{"cell_type":"markdown","metadata":{"id":"j1mpp7RXhg5_"},"source":["Como pode ser visto, uma lista de tuplas (o token e a tag POS) é retornada pelo tagger POS. Agora, os tags POS precisam ser convertidos em um formato que possa ser entendido pelo lematizador rdNet e enviado como entrada junto com os tokens.\n","\n","O trecho de código faz o que é necessário mapeando as tags POS para o primeiro caractere, que é aceito pelo lematizador no formato apropriado:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1KdQciChg5_"},"outputs":[],"source":["from nltk.corpus import wordnet\n","\n","##This is a common method which is widely used across the NLP community of practitioners and readers\n","\n","def get_part_of_speech_tags(token):\n","    \n","    \"\"\"Maps POS tags to first character lemmatize() accepts.\n","    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n","\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    \n","    tag = nltk.pos_tag([token])[0][1][0].upper()\n","    \n","    return tag_dict.get(tag, wordnet.NOUN)"]},{"cell_type":"markdown","metadata":{"id":"DDnqfHp-hg6A"},"source":["## Wordnet Lemmatizer with POS Tag Information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9yiTuuvhg6B","outputId":"6c3e9e8d-09d7-43ec-cc03-98d7d3b2a697"},"outputs":[{"name":"stdout","output_type":"stream","text":["We be put in effort to enhance our understand of Lemmatization\n"]}],"source":["lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n","print(' '.join(lemmatized_output_with_POS_information))"]},{"cell_type":"markdown","metadata":{"id":"F96ERZeUhg6B"},"source":["As seguintes conversões aconteceram:\n","\n","are **to** be\n","\n","putting **to** put\n","\n","efforts **to** effort\n","\n","understanding **to** understand"]},{"cell_type":"markdown","metadata":{"id":"tKjI4Cc-hg6C"},"source":["## Lemmatization vs Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb4wm91whg6C","outputId":"d48725fc-d1ec-427c-bae0-6d5b30e28f36"},"outputs":[{"name":"stdout","output_type":"stream","text":["we are put in effort to enhanc our understand of lemmat\n"]}],"source":["stemmer2 = SnowballStemmer(language='english')\n","stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n","print(' '.join(stemmed_sentence))"]},{"cell_type":"markdown","metadata":{"id":"nnAMmc8khg6E"},"source":["Como pode ser visto, o lematizador WordNet faz uma conversão sensata e com base no contexto do token em sua forma básica, ao contrário do stemmer, que tenta separar os afixos do token"]},{"cell_type":"markdown","metadata":{"id":"IyTTjUdJhg6H"},"source":["# spaCy Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZsfgqd_hg6J"},"outputs":[],"source":["#!pip install spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hezaA0_Whg6K","outputId":"feae4a8c-d7a6-4781-f142-7d54341f83dc"},"outputs":[{"ename":"OSError","evalue":"[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-d002aeba34f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We are putting in efforts to enhance our understanding of Lemmatization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."]}],"source":["import spacy\n","nlp = spacy.load('en')\n","doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n","\" \".join([token.lemma_ for token in doc])"]},{"cell_type":"markdown","metadata":{"id":"qiZqBLdfhg6M"},"source":["# Stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuPMv0Izhg6M","outputId":"2d92efb7-0da0-48c3-ab8d-01ac10818159"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/amankedia/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["\"it's, yours, an, doing, any, mightn't, you, having, wasn't, themselves, just, over, below, needn't, a, this, shan't, them, isn't, was, wouldn't, as, only, his, or, shan, wouldn, don, where, own, were, he, out, do, it, am, won, isn, there, hers, to, ll, most, for, weren, have, by, while, the, re, that, down, haven, has, is, here, itself, all, didn, herself, shouldn, him, ve, who, doesn, m, hadn't, after, further, weren't, at, hadn, should've, too, because, can, now, same, more, she's, wasn, these, yourself, himself, being, very, until, myself, few, so, which, ourselves, they, t, you'd, did, o, aren, but, that'll, such, whom, of, s, you'll, those, doesn't, my, what, aren't, during, hasn, through, will, couldn, i, mustn, needn, mustn't, d, had, me, under, won't, haven't, its, with, when, their, between, if, once, against, before, on, not, you're, each, yourselves, in, and, are, shouldn't, some, nor, her, does, she, off, how, both, our, then, why, again, we, no, y, be, other, ma, from, up, theirs, couldn't, should, into, didn't, ours, about, ain, you've, don't, above, been, than, your, hasn't, mightn\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop = set(stopwords.words('english'))\n","\", \".join(stop)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbQtO3Hqhg6N","outputId":"ddc4f18e-1084-403a-ab1c-f83caeceb2cd"},"outputs":[{"data":{"text/plain":["'how putting efforts enhance understanding Lemmatization'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n","\n","stop = set(stopwords.words('english'))\n","\n","sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n","\n","for word in wh_words:\n","    stop.remove(word)\n","\n","sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n","\" \".join(sentence_after_stopword_removal)"]},{"cell_type":"markdown","metadata":{"id":"c7ZB_29ahg6P"},"source":["# Case Folding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWmKg9DKhg6S","outputId":"e422fe24-7c13-4b16-8f6d-6e5c68e487e1"},"outputs":[{"data":{"text/plain":["'we are putting in efforts to enhance our understanding of lemmatization'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n","s = s.lower()\n","s"]},{"cell_type":"markdown","metadata":{"id":"MxgCbh0Lhg6T"},"source":["# N-grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSRVZRW6hg6T","outputId":"0730f3ba-2e9e-4054-8f5e-2f870fce330a"},"outputs":[{"data":{"text/plain":["['Natural Language',\n"," 'Language Processing',\n"," 'Processing is',\n"," 'is the',\n"," 'the way',\n"," 'way to',\n"," 'to go']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.util import ngrams\n","s = \"Natural Language Processing is the way to go\"\n","tokens = s.split()\n","bigrams = list(ngrams(tokens, 2))\n","[\" \".join(token) for token in bigrams]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjcFqomnhg6U","outputId":"cef7f83f-e57c-45fb-b5fe-ed29ab0acea6"},"outputs":[{"data":{"text/plain":["['Natural Language Processing',\n"," 'Language Processing is',\n"," 'Processing is the',\n"," 'is the way',\n"," 'the way to',\n"," 'way to go']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["s = \"Natural Language Processing is the way to go\"\n","tokens = s.split()\n","trigrams = list(ngrams(tokens, 3))\n","[\" \".join(token) for token in trigrams]"]},{"cell_type":"markdown","metadata":{"id":"vEA2EEKxhg6V"},"source":["# Building a basic vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsoywWtUhg6V","outputId":"e8e22bc3-5151-4c33-94b5-ae866d8268c8"},"outputs":[{"data":{"text/plain":["['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["s = \"Natural Language Processing is the way to go\"\n","tokens = set(s.split())\n","vocabulary = sorted(tokens)\n","vocabulary"]},{"cell_type":"markdown","metadata":{"id":"H5u-s1UEhg6V"},"source":["# Removing HTML Tags"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDi3w4eEhg6W","outputId":"4345e64e-333c-47d3-f1fb-ae6d3ac48f16"},"outputs":[{"name":"stdout","output_type":"stream","text":["My First HeadingMy first paragraph.\n"]}],"source":["html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n","from bs4 import BeautifulSoup\n","\n","soup = BeautifulSoup(html)\n","text = soup.get_text()\n","print(text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Stemming, Lemmatization, Stopword Removal, Case-Folding, N-grams and HTML tags.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}