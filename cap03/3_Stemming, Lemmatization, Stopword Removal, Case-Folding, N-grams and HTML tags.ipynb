{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding word normalization\n",
    "@author: Aman Kedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na maioria das vezes, não queremos ter cada fragmento de palavra individual que já encontramos em nosso vocabulário. Poderíamos desejar isso por várias razões, uma delas é a necessidade de distinguir corretamente (por exemplo) a frase U.N. (com caracteres separados por um ponto) de UN (sem pontos). Também podemos trazer palavras à sua forma raiz no dicionário. Por exemplo, **am, are e is** podem ser identificados por sua forma raiz, **be**. Em outra frente, podemos remover as inflexões das palavras para trazê-las à mesma forma.\n",
    "\n",
    "As palavras **car, cars  e car's** podem ser identificadas como **car**.\n",
    "\n",
    "Além disso, palavras comuns que ocorrem com muita frequência e não transmitem muito significado, como os artigos **a, an e the,** podem ser removidas. \n",
    "\n",
    "No entanto, tudo isso depende muito dos casos de uso. Palavras como **(when, why, where, and who)Wh-words**, por exemplo, quando, por que, onde e quem, não carregam muitas informações na maioria dos contextos e são removidos como parte de uma técnica chamada **stopword removal** remoção de palavras irrelevantes, que veremos um pouco mais tarde na seção Remoção de palavras irrelevantes; no entanto, em situações como classificação e resposta de perguntas, essas palavras tornam-se muito importantes e não devem ser removidas. Agora, com uma compreensão básica dessas técnicas, vamos nos aprofundar nelas em detalhe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Imagine reunir todas as palavras **computador, computadorização e computadorizar** em uma palavra, computador. O que acontece aqui é chamado de **stemming** derivação. Como parte da stemming, uma tentativa grosseira é feita para remover as formas flexionais de uma palavra e trazê-las para uma forma básica chamada radical. As partes cortadas são chamadas de afixos. No exemplo anterior, compute é a forma básica e os afixos são r, rization e rize, respectivamente. Uma coisa a ter em mente é que o radical não precisa ser uma palavra válida como a conhecemos. Por exemplo,\n",
    "a palavra tradicional seria derivada de tradit, que não é uma palavra válida no dicionário inglês.\n",
    "\n",
    "Os dois algoritmos / métodos mais comuns empregados para o stemming  incluem o Porter stemmer  e o Snowball stemmer. O  Porter oferece suporte para o idioma inglês, enquanto o  Snowball, que é um aprimoramento do  Porter, oferece suporte a vários idiomas, o que pode ser visto no seguinte trecho de código e sua saída:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma coisa a se notar no trecho é que o Porter é uma das opçoes providas pela Snowball.Outros stemmers incluem Lancaster, Dawson, Krovetz e lematizadores Lovins, entre outros. Veremos os stemmer Porter e Snowball em detalhes aqui. O stemmer Porter funciona apenas com strings, enquanto o stemmer Snowball funciona com strings e dados Unicode. O stemmer Snowball também permite a opção de ignorar palavras irrelevantes como uma funcionalidade inerente. Agora, vamos primeiro aplicar o stemmer Porter às palavras e ver seus efeitos no seguinte bloco de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plurals = ['ate', 'eat', 'eaten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
    "           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress, fli, die, mule, die, agre, own, humbl, size, meet, state, siez, item, tradit, refer, colon, plot, have, gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(', '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress, fli, die, mule, die, agre, own, humbl, size, meet, state, siez, item, tradit, refer, colon, plot, have, generous\n"
     ]
    }
   ],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "singles = [stemmer2.stem(plural) for plural in plurals]\n",
    "print(', '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatização\n",
    "\n",
    "Ao contrário da stemming, em que alguns caracteres são removidos das palavras usando métodos rudes, a lematização é um processo em que o contexto é usado para converter uma palavra em sua forma de base significativa. Ajuda a agrupar palavras que têm uma forma base comum e, portanto, podem ser identificadas como um único item. A forma de base é conhecida como lema da palavra e às vezes também é conhecida como forma de dicionário.\n",
    "\n",
    "Os algoritmos de lematização tentam identificar a forma do lemma de uma palavra levando em consideração o contexto da proximidade da palavra, as marcas da classe gramatical (**parto of speech**POS), o significado de uma palavra e assim por diante. A vizinhança pode abranger palavras na vizinhança, frases ou até mesmo documentos.\n",
    "\n",
    "Além disso, as mesmas palavras podem ter lemas diferentes dependendo do contexto. Um lematizador tentaria identificar as marcas de classe gramatical com base no contexto para identificar o lema apropriado. O lematizador mais comumente usado é o lematizador WordNet. Outros lematizadores incluem o  Spacy, o TextBlob e o  Gensim, entre outros. Nesta seção, exploraremos os lematizadores WordNet e Spacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/carlos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
      "The lemmatized output is:  We, are, putting, in, effort, to, enhance, our, understanding, of, Lemmatization\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "token_list = s.split()\n",
    "print(\"The tokens are: \", token_list)\n",
    "lemmatized_output = ', '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
    "print(\"The lemmatized output is: \", lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O lematizador WordNet funciona bem se as tags POS também forem fornecidas como entradas.\n",
    "\n",
    "É realmente impossível anotar manualmente cada palavra com sua tag POS em um corpus de texto.\n",
    "Agora, como resolvemos esse problema e fornecemos as marcas de classe gramatical para palavras individuais como entrada para o lematizador do WordNet?\n",
    "\n",
    "Felizmente, a biblioteca nltk fornece um método para localizar tags POS para uma lista de palavras usando um tagger perceptron médio, **cujos detalhes estão fora do escopo deste capítulo.**\n",
    "\n",
    "As tags POS para a frase  **We are trying our best to understand Lemmatization** pode ser encontrada no seguinte trecho de código:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/carlos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('putting', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('enhance', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('understanding', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Lemmatization', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(token_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tag Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto, uma lista de tuplas (o token e a tag POS) é retornada pelo tagger POS. Agora, os tags POS precisam ser convertidos em um formato que possa ser entendido pelo lematizador rdNet e enviado como entrada junto com os tokens.\n",
    "\n",
    "O trecho de código faz o que é necessário mapeando as tags POS para o primeiro caractere, que é aceito pelo lematizador no formato apropriado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
    "\n",
    "def get_part_of_speech_tags(token):\n",
    "    \n",
    "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
    "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Lemmatizer with POS Tag Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'putting',\n",
       " 'in',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'enhance',\n",
       " 'our',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'Lemmatization']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([token_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We be put in effort to enhance our understand of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
    "print(' '.join(lemmatized_output_with_POS_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seguintes conversões aconteceram:\n",
    "\n",
    "are **to** be\n",
    "\n",
    "putting **to** put\n",
    "\n",
    "efforts **to** effort\n",
    "\n",
    "understanding **to** understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization vs Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are put in effort to enhanc our understand of lemmat\n"
     ]
    }
   ],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
    "print(' '.join(stemmed_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto, o lematizador WordNet faz uma conversão sensata e com base no contexto do token em sua forma básica, ao contrário do stemmer, que tenta separar os afixos do token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O lematizador Spacy vem com modelos pré-treinados que podem analisar o texto e descobrir as várias propriedades do texto, como tags POS, tags de entidade nomeada e assim por diante, com uma simples chamada de função. Os modelos pré-construídos identificam as tags POS e atribuem um lema a cada token, diferentemente do lematizador WordNet, onde as tags POS precisam ser fornecidas explicitamente.\n",
    "Podemos instalar o Spacy e baixar o modelo en para o idioma inglês executando o seguinte comando na linha de comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli \n",
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-PRON- be put in effort to enhance -PRON- understanding of Lemmatization'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O lematizador Spacy executou um trabalho decente sem as informações de entrada das tags POS. A vantagem aqui é que não há necessidade de procurar dependências externas para buscar tags POS, pois as informações são incorporadas ao modelo pré-treinado.\n",
    "Outra coisa a ser observada na saída anterior é o -PRON-lema. O lema para Pronomes é retornado como -PRON- no comportamento padrão do Spacy. Ele pode atuar como um recurso ou, inversamente, pode ser uma limitação, já que o lema exato não está sendo retornado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are stopwords?\n",
    "Stopwords are words such as a, an, the, in, at, and so on that occur frequently in text corpora\n",
    "and do not carry a lot of information in most contexts. These words, in general, are required\n",
    "for the completion of sentences and making them grammatically sound. They are often the\n",
    "most common words in a language and can be filtered out in most NLP tasks, and\n",
    "consequently help in reducing the vocabulary or search space. There is no single list of\n",
    "stopwords that is available universally, and they vary mostly based on use cases; however,\n",
    "a certain list of words is maintained for languages that can be treated as stopwords specific\n",
    "to that language, but they should be modified based on the problem that is being solved.\n",
    "Let’s look at the stopwords available for English in the nltk library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/carlos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"same, hadn't, there, his, then, haven, until, didn't, only, down, y, me, their, yours, that'll, weren't, from, very, some, himself, has, once, few, wasn't, those, above, that, below, mustn, on, these, which, too, mightn't, against, had, re, so, s, such, was, needn't, being, off, to, you've, o, is, doesn't, while, shouldn, not, isn, ma, should, they, don, couldn't, theirs, your, you're, for, did, because, this, further, at, about, haven't, my, does, myself, what, are, him, having, i, it, hasn't, herself, am, if, during, all, weren, wouldn, through, before, been, over, he, more, aren, between, a, ain, can, other, mightn, themselves, of, mustn't, but, under, have, our, should've, whom, t, you, an, hers, d, here, will, ll, didn, she, the, shan, when, yourself, again, own, as, where, shan't, now, isn't, doesn, she's, don't, shouldn't, ve, we, itself, after, couldn, with, them, ourselves, you'd, and, won, do, wasn, who, or, nor, hadn, how, up, be, out, any, than, m, her, needn, why, its, were, you'll, both, in, doing, into, most, each, it's, yourselves, wouldn't, hasn, no, ours, by, aren't, just, won't\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s look at the stopwords available for English in the nltk library!\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\", \".join(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você olhar de perto, notará que WH- palavras como who , what , when , why , how , which , where e who fazem parte desta lista de palavras irrelevantes; no entanto, em uma das seções anteriores, foi mencionado que essas palavras são muito significativas em casos de uso, como resposta a perguntas e classificação de perguntas. Devem ser tomadas medidas para garantir que essas palavras não sejam filtradas quando o corpus de texto for submetido à remoção de palavras irrelevantes. Vamos aprender como isso\n",
    "pode ser obtido executando-se o seguinte bloco de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how putting efforts enhance understanding Lemmatization'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
    "\n",
    "for word in wh_words:\n",
    "    stop.remove(word)\n",
    "\n",
    "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
    "\" \".join(sentence_after_stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho de código anterior mostra que a frase  \"how are we putting in efforts to enhance our understanding of Lemmatization\" é modificada para 'how putting efforts enhance understanding Lemmatization'. As palavras irrelevantes são , we , in , to , our , e of foram removidas da frase. A remoção de palavras irrelevantes geralmente é a primeira etapa tomada após a tokenização durante a construção de um vocabulário ou o pré-processamento de dados de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uutra estratégia que ajuda na normalização é chamada de case folding. Como parte deste, todas as letras no corpus de texto são convertidas em minúsculas. O e o serão tratados da mesma forma em um cenário de case folding, enquanto seriam tratados de forma diferente em um cenário de não case folding. Essa técnica auxilia sistemas que lidam com recuperação de informações, como os mecanismos de busca.\n",
    "Lamborghini, que é um nome próprio, será tratado como lamborghini; se o usuário digitou Lamborghini ou lamborghini não faria diferença, e os mesmos resultados seriam retornados.\n",
    "No entanto, em situações em que os nomes próprios são derivados de termos nominais comuns, a dobragem de maiúsculas se tornará um gargalo, pois a distinção baseada em casos se tornará uma característica importante aqui. Por exemplo, a General Motors é composta de termos substantivos comuns, mas ela própria é um substantivo próprio. Realizar a dobragem da caixa aqui pode causar problemas. Outro problema é quando as siglas são convertidas para minúsculas. Há uma grande chance de que eles sejam mapeados para substantivos comuns. Um exemplo amplamente utilizado aqui é o CAT, que significa Teste de Admissão Comum na Índia, sendo convertido em gato.\n",
    "Uma possível solução para isso é construir modelos de aprendizado de máquina que possam usar recursos de uma frase para determinar quais palavras ou tokens na frase devem ser minúsculos e quais não devem ser; no entanto, essa abordagem nem sempre ajuda quando os usuários digitam principalmente em letras minúsculas. Como resultado, colocar tudo em minúsculas se torna uma solução sábia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linguagem aqui é uma característica importante; em alguns idiomas, como o inglês, a capitalização ponto a ponto em um texto carrega muitas informações, enquanto em alguns outros idiomas, os casos podem não ser tão importantes.\n",
    "O trecho de código a seguir mostra uma abordagem muito direta que converteria todas as letras de uma frase em minúsculas, fazendo uso do método lower() disponível em Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are putting in efforts to enhance our understanding of lemmatization'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language',\n",
       " 'Language Processing',\n",
       " 'Processing is',\n",
       " 'is the',\n",
       " 'the way',\n",
       " 'way to',\n",
       " 'to go']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "[\" \".join(token) for token in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing',\n",
       " 'Language Processing is',\n",
       " 'Processing is the',\n",
       " 'is the way',\n",
       " 'the way to',\n",
       " 'way to go']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "[\" \".join(token) for token in trigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a basic vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = set(s.split())\n",
    "vocabulary = sorted(tokens)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload: My First HeadingMy first paragraph.\n"
     ]
    }
   ],
   "source": [
    "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "text = soup.get_text()\n",
    "print('payload:', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
