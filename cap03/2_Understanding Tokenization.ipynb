{"cells":[{"cell_type":"markdown","metadata":{"id":"tGd26G2Cp6-E"},"source":["# Exploring Tokenization\n","\n","@author: Aman Kedia\n"]},{"cell_type":"markdown","metadata":{"id":"MjkfoQM1p6-I"},"source":["Para construir um vocabulário, a primeira coisa a fazer é quebrar os documentos ou frases em blocos chamados tokens. Cada token carrega um significado semântico associado a ele. A tokenização é uma das coisas fundamentais a fazer em qualquer atividade de processamento de texto.\n","A tokenização pode ser considerada uma técnica de segmentação em que você tenta quebrar pedaços maiores de pedaços de texto em pedaços menores e significativos. Tokens em geral\n","compreendem palavras e números, mas podem ser estendidos para incluir sinais de pontuação, símbolos e, às vezes, emoticons compreensíveis."]},{"cell_type":"markdown","metadata":{"id":"_u7Bqxmfp6-J"},"source":["### Exemplos"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-xOW0RXp6-J","executionInfo":{"status":"ok","timestamp":1639698654302,"user_tz":180,"elapsed":4,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"f2e7351d-7783-457a-9779-094ada7defd8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['The', 'capital', 'of', 'China', 'is', 'Beijing']"]},"metadata":{},"execution_count":1}],"source":["sentence = \"The capital of China is Beijing\"\n","sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"5w1QxS29p6-L"},"source":["### Problemas com tokenização\n","Considere a frase e a divisão correspondente no exemplo a seguir:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9z2juZcp6-L","executionInfo":{"status":"ok","timestamp":1639698690460,"user_tz":180,"elapsed":227,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"d47b482c-0215-4691-967c-4f25837b362f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"China's\", 'capital', 'is', 'Beijing']"]},"metadata":{},"execution_count":2}],"source":["sentence = \"China's capital is Beijing\"\n","sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"5XC-8NGYp6-M"},"source":["No exemplo anterior, deveria ser China, Chinas ou China's? Um método de divisão geralmente não sabe como lidar com situações que contêm apóstrofos.\n","\n","Nos próximos dois exemplos, como lidamos com **we'll** e **I'm**? **We'll** indica **we will** e **I'm** indica **I am**. Qual deve ser a forma tokenizada de **we'll**? Deve ficar **well ou we'll ou we e 'll** separadamente? Da mesma forma, como simbolizamos **I'm**? Uma tokenização ideal deve ser capaz de processar **we'll** em dois tokens, ***we e will**, e **I'm** em em dois tokens, **I e am**. Vamos ver como nosso método de divisão se sairia nessa situação. "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWiNX7Jwp6-M","executionInfo":{"status":"ok","timestamp":1639698707022,"user_tz":180,"elapsed":248,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"e4b1a572-a5d7-42bd-83fe-062e60b6d54d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Beijing', 'is', 'where', \"we'll\", 'go']"]},"metadata":{},"execution_count":3}],"source":["sentence = \"Beijing is where we'll go\"\n","sentence.split()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irMA5Cpep6-N","executionInfo":{"status":"ok","timestamp":1639698709113,"user_tz":180,"elapsed":242,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"38d838aa-42c2-4937-ea31-03fa94966abe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"I'm\", 'going', 'to', 'travel', 'to', 'Beijing']"]},"metadata":{},"execution_count":4}],"source":["sentence = \"I'm going to travel to Beijing\"\n","sentence.split()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9MefCgWp6-O","executionInfo":{"status":"ok","timestamp":1639698710447,"user_tz":180,"elapsed":225,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"a5826a39-57c2-4765-eb72-4523c04e513d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Most', 'of', 'the', 'times', 'umm', 'I', 'travel']"]},"metadata":{},"execution_count":5}],"source":["sentence = \"Most of the times umm I travel\"\n","sentence.split()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfeqjmFVp6-O","executionInfo":{"status":"ok","timestamp":1639698778074,"user_tz":180,"elapsed":222,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"459c82bd-e7fe-487e-e10d-bb0dfab2de1b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing']"]},"metadata":{},"execution_count":6}],"source":["sentence = \"Let's travel to Hong Kong from Beijing\"\n","sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"wv1KSaNap6-P"},"source":["Aqui, idealmente, Hong Kong deveria ser um token, mas pense em outra frase: O nome do Rei é Kong. Em tais cenários, Kong deve ser um token individual. Em tais situações, o contexto pode desempenhar um papel importante na compreensão de como tratar representações de token semelhantes quando o contexto varia. Tokens de tamanho 1, como Kong, são chamados de unigramas, enquanto tokens de tamanho 2, como Hong Kong, são chamados de bigramas. Eles podem ser generalizados sob a asa de n-gramas, que discutiremos no final deste capítulo.\n","\n","Como lidamos com os **periods**? Como entendemos se significam o fim de uma frase ou indicam uma abreviatura?\n","No seguinte trecho de código e na saída subsequente, o ponto entre M e S é, na verdade, indicativo de uma abreviatura:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE7UciUBp6-P","executionInfo":{"status":"ok","timestamp":1639698890768,"user_tz":180,"elapsed":213,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"aeb1c278-56f9-457f-c3bf-27dd8e954f6b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing']"]},"metadata":{},"execution_count":7}],"source":["sentence = \"A friend is pursuing his M.S from Beijing\"\n","sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"hqlDVlTbp6-Q"},"source":["No próximo exemplo, um token como **umm** tem algum significado? Não deveria ser removido?\n","Mesmo que um token como **umm** não faça parte do vocabulário em inglês, torna-se importante nos casos de uso em que a síntese de fala está envolvida, pois indica que a pessoa está tomando uma pausa aqui e tente pensar em algo. Novamente, assim como o contexto, a noção de caso de uso também importa ao entender onde algo deve ser tokenizado ou simplesmente removido como um fragmento de texto que não transmite nenhum significado:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8oinZmmKp6-Q","executionInfo":{"status":"ok","timestamp":1639698965730,"user_tz":180,"elapsed":237,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"7734beb3-7193-41bd-a4b9-7f3a2a0a0f17"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Most', 'of', 'the', 'times', 'umm', 'I', 'travel']"]},"metadata":{},"execution_count":8}],"source":["sentence = \"Most of the times umm I travel\"\n","sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"6C3iV_VUp6-Q"},"source":["A ascensão das plataformas de mídia social resultou em um influxo maciço de dados do usuário, que é uma rica mina de informações para entender indivíduos e comunidades; no entanto, também contribuiu para o surgimento de um mundo de emoticons, formas curtas, novas abreviações (frequentemente chamadas de linguagem milenar) e assim por diante. É necessário entender esse tipo de texto sempre crescente, bem como aqueles casos em que, por exemplo, um caractere P usado com dois pontos (:) e hífen\n","(-) denota um rosto com a língua para fora. Hashtags são outra coisa muito comum nas mídias sociais que são principalmente indicativas de resumos ou emoções por trás de uma postagem no Facebook ou um tweet no Twitter. Um exemplo disso é mostrado no exemplo a seguir. Esse crescimento leva ao desenvolvimento de tokenizers como TweetTokenizer:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QC_Dw9qHp6-R","executionInfo":{"status":"ok","timestamp":1639699034896,"user_tz":180,"elapsed":243,"user":{"displayName":"Fellipe silva martins","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZsSow9ZKbLQt0qtcp6cJf6EBQXzUJrz3xw4O7UA=s64","userId":"06403637826723224523"}},"outputId":"7c2f2333-be97-4853-e909-890b7fefa839"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome']"]},"metadata":{},"execution_count":9}],"source":["sentence = \"Beijing is a cool place!!! :-P <3 #Awesome\"\n","sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"nnQC44ccp6-R"},"source":["Na próxima seção, veremos TweetTokenizer e alguns outros tokenizadores padrão disponíveis na biblioteca nltk"]},{"cell_type":"markdown","metadata":{"id":"k59eeyvZp6-S"},"source":["### Diferentes tipos de tokenizadores\n","\n","Com base no entendimento que desenvolvemos até agora, vamos discutir os diferentes tipos de tokenizadores que estão prontamente disponíveis para uso e ver como eles podem ser aproveitados para a tokenização adequada de texto."]},{"cell_type":"markdown","metadata":{"id":"tzU7M7a1p6-S"},"source":["### Expressões regulares\n","\n","Expressões regulares são sequências de caracteres que definem um padrão de pesquisa. Eles são um dos primeiros e ainda são uma das ferramentas mais eficazes para identificar padrões em texto.\n","\n","Imagine pesquisar por IDs de e-mail em um corpus de texto. Eles seguem o mesmo padrão e são guiados por um conjunto de regras, independentemente do domínio em que estejam hospedados. As expressões regulares são o caminho a percorrer para identificar essas coisas em dados de texto, em vez de experimentar técnicas orientadas para o aprendizado de máquina. Outros exemplos notáveis onde expressões regulares foram amplamente empregadas incluem  SUTime oferecida pela Stanford NLP, em que a tokenização com base em expressões regulares é usada para identificar a data, hora, duração e\n","definir entidades de tipo no texto. Observe a seguinte frase:"]},{"cell_type":"markdown","metadata":{"id":"XHfJbJDNp6-S"},"source":["Last summer, they met every Tuesday afternoon, from 1:00 pm to 3:00 pm."]},{"cell_type":"markdown","metadata":{"id":"rZlXoYoSp6-S"},"source":["Para esta frase, a biblioteca SUTime retornaria expressões TIMEX onde cada expressão TIMEX indicaria a existência de uma das entidades acima mencionadas:"]},{"cell_type":"markdown","metadata":{"id":"IqOrJk-Np6-T"},"source":["Last summer  ** 2019-SU**  <TIMEX3 tid=\"t1\" type=\"DATE\" value=\"2019-SU\">Last summer</TIMEX3>\n","\n","every Tuesday afternoon **XXXX-WXX-2TAF**  <TIMEX3 periodicity=\"P1W\" quant=\"every\"tid=\"t2\" type=\"SET\" value=\"XXXX-WXX-2TAF\">every Tuesday afternoon</TIMEX3>\n","\n","1:00 pm  **2020-01-06T13:00 **<TIMEX3 tid=\"t3\" type=\"TIME\" value=\"2020-01-06T13:00\">1:00 pm</TIMEX3>\n","\n","3:00 pm **2020-01-06T15:00** <TIMEX3 tid=\"t4\" type=\"TIME\" value=\"2020-01-06T15:00\">3:00 pm</TIMEX3>"]},{"cell_type":"markdown","metadata":{"id":"wyowKKxAp6-T"},"source":["### Tokenizadores baseados em expressões regulares\n","\n","O pacote nltk em Python fornece um método baseado em expressões regulares\n","funcionalidade de tokenizers (RegexpTokenizer). Ele pode ser usado para tokenizar ou dividir uma frase com base em uma expressão regular fornecida. Faça a seguinte frase:\n","\n","*Um relógio Rolex custa na faixa de $ 3.000,0 - $ 8.000,0 nos EUA.*\n","\n","Aqui, gostaríamos de ter expressões indicando dinheiro, sequências alfabéticas e abreviações juntas. Podemos definir uma expressão regular para fazer isso e passar a expressão ao objeto tokenizer correspondente, conforme mostrado no seguinte bloco de código:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBrvm3Kyp6-T","outputId":"05df3018-f17a-4d06-ecb0-d1ef3caeacfd"},"outputs":[{"data":{"text/plain":["['A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '$3000.0',\n"," '-',\n"," '$8000.0',\n"," 'in',\n"," 'USA',\n"," '.']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import RegexpTokenizer\n","s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n","tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n","tokenizer.tokenize(s)"]},{"cell_type":"markdown","metadata":{"id":"XI7SAdKBp6-T"},"source":["# Regex Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"gVplZyrvp6-U"},"source":["Agora, como isso funciona:\n","\n","A expressão regular \\w +|\\$[\\d\\.] + |\\S + permite três padrões alternativos:\n","\n","Primeira alternativa: \\w + que casa com qualquer caractere de palavra (igual a [a-zA-Z0-9_]).\n","O + é um quantificador e corresponde entre uma e ilimitadas vezes tantas vezes quanto possível.\n","\n","Segunda alternativa: \\$[\\d\\.] +. Aqui, \\$ corresponde ao caractere $, \\d corresponde a um dígito entre 0 e 9, \\.corresponde ao.(ponto final) e + novamente atua como um quantificador combinando entre um e um número ilimitado de vezes.\n","\n","Terceira alternativa: \\S +. Aqui, \\S aceita qualquer caractere diferente de espaço em branco e + novamente atua da mesma maneira que nas duas alternativas anteriores.\n","\n","Existem outros tokenizers construídos sobre o RegexpTokenizer, como o tokenizer BlankLine, que simboliza uma string que trata as linhas em branco como delimitadores, onde as linhas em branco são aquelas que não contêm caracteres, exceto espaços ou tabulações.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yJoHiOUUp6-U"},"source":["# Blankline Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AR-MoxJip6-U","outputId":"c3277603-2a2e-4c46-b739-aa43ac6f6b41"},"outputs":[{"data":{"text/plain":["['A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.',\n"," 'I want a book as well']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import BlanklineTokenizer\n","s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n\\n I want a book as well\"\n","tokenizer = BlanklineTokenizer()\n","tokenizer.tokenize(s)"]},{"cell_type":"markdown","metadata":{"id":"9DQGjnwhp6-U"},"source":["O tokenizer WordPunct é outra implementação no topo do RegexpTokenizer, que transforma um texto em uma sequência de caracteres alfabéticos e não alfabéticos usando a expressão regular \\w+|[^ \\ w \\ s] +."]},{"cell_type":"markdown","metadata":{"id":"jgUeYGD_p6-U"},"source":["# WordPunct Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HANq3CCHp6-V","outputId":"7a31831d-9e79-4948-ff68-7b2e1dfbc176"},"outputs":[{"data":{"text/plain":["['A',\n"," 'Rolex',\n"," 'watch',\n"," 'costs',\n"," 'in',\n"," 'the',\n"," 'range',\n"," 'of',\n"," '$',\n"," '3000',\n"," '.',\n"," '0',\n"," '-',\n"," '$',\n"," '8000',\n"," '.',\n"," '0',\n"," 'in',\n"," 'USA',\n"," '.',\n"," 'I',\n"," 'want',\n"," 'a',\n"," 'book',\n"," 'as',\n"," 'well']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import WordPunctTokenizer\n","s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n I want a book as well\"\n","tokenizer = WordPunctTokenizer()\n","tokenizer.tokenize(s)"]},{"cell_type":"markdown","metadata":{"id":"gGXtXgbZp6-V"},"source":["# TreebankWord Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"vFtXJwv_p6-V"},"source":["O tokenizer Treebank também usa expressões regulares para tokenizar texto de acordo com o Penn Treebank (https: // catalog. Ldc. Upenn. Edu / docs / LDC95T7 / cl93. Html). \n","Aqui, as palavras são divididas principalmente com base na pontuação.\n","\n","O tokenizer Treebank faz um ótimo trabalho em dividir as contrações como **doesn't** para **does** e **n't**. Além disso, identifica os pontos finais das linhas e os elimina. A pontuação, como vírgulas, é dividida se for seguida por espaços em branco.\n","\n","Vejamos a seguinte frase e tokenize-a usando o tokenizer Treebank:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tenC66Fp6-V","outputId":"777820a0-0c16-4ea9-dbf5-d7dafe4404bb"},"outputs":[{"data":{"text/plain":["['I',\n"," \"'m\",\n"," 'going',\n"," 'to',\n"," 'buy',\n"," 'a',\n"," 'Rolex',\n"," 'watch',\n"," 'which',\n"," 'does',\n"," \"n't\",\n"," 'cost',\n"," 'more',\n"," 'than',\n"," '$',\n"," '3000.0']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import TreebankWordTokenizer\n","s = \"I'm going to buy a Rolex watch which doesn't cost more than $3000.0\"\n","tokenizer = TreebankWordTokenizer()\n","tokenizer.tokenize(s)"]},{"cell_type":"markdown","metadata":{"id":"rjZJb68Zp6-V"},"source":["Como pode ser visto no exemplo e na saída correspondente, este tokenizer ajuda principalmente na análise de cada componente no texto separadamente. O **I'm** é dividido em dois componentes, a saber, o I, que corresponde a um sintagma nominal, e o 'm, que corresponde a um componente do verbo. Essa divisão nos permite trabalhar com tokens individuais que carregam informações significativas que seriam difíceis de analisar e analisar se fosse um único token.\n","Da mesma forma, **doesn't** é dividido em **does** e **n't**, ajudando a analisar e compreender melhor a semântica inerente associada ao **n't**, que indica a negação."]},{"cell_type":"markdown","metadata":{"id":"eM780xl_p6-W"},"source":["# Tweet Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"aMSL12XQp6-W"},"source":["Conforme discutido anteriormente, o surgimento das mídias sociais deu origem a uma linguagem informal em que as pessoas marcam umas às outras usando seus identificadores de mídia social e usam muitos emoticons, hashtags e textos abreviados para se expressarem. Precisamos de tokenizadores que possam analisar esse texto e tornar as coisas mais compreensíveis. TweetTokenizer atende a este caso de uso significativamente. Vejamos a seguinte frase / tweet:"]},{"cell_type":"markdown","metadata":{"id":"X7C5S2oTp6-W"},"source":["**@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex\n","<3**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VusX_6r2p6-W","outputId":"5393f50d-abbc-4aa7-b51d-37abd07fdfa9"},"outputs":[{"data":{"text/plain":["['@amankedia',\n"," \"I'm\",\n"," 'going',\n"," 'to',\n"," 'buy',\n"," 'a',\n"," 'Rolexxxxxxxx',\n"," 'watch',\n"," '!',\n"," '!',\n"," '!',\n"," ':-D',\n"," '#happiness',\n"," '#rolex',\n"," '<3']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import TweetTokenizer\n","s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n","tokenizer = TweetTokenizer()\n","tokenizer.tokenize(s)"]},{"cell_type":"markdown","metadata":{"id":"kFuM74ftp6-W"},"source":["Outra coisa comum com a escrita em mídia social é o uso de expressões como Rolexxxxxxxx. Aqui, muitos x's estão presentes além do normal; é uma tendência muito comum e deve ser tratada para que seja o mais próximo possível do normal.\n","O TweetTokenizer fornece dois parâmetros adicionais na forma de reduce_len, que tenta reduzir o excesso de caracteres em um token. A palavra Rolexxxxxxxx é na verdade tokenizada como Rolexxx em uma tentativa de reduzir o número de x's presentes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWXcuLgNp6-W","outputId":"91aac8b0-0566-4a7f-d165-5928ac6ea8ae"},"outputs":[{"data":{"text/plain":["[\"I'm\",\n"," 'going',\n"," 'to',\n"," 'buy',\n"," 'a',\n"," 'Rolexxx',\n"," 'watch',\n"," '!',\n"," '!',\n"," '!',\n"," ':-D',\n"," '#happiness',\n"," '#rolex',\n"," '<3']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import TweetTokenizer\n","s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n","tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n","tokenizer.tokenize(s)"]},{"cell_type":"markdown","metadata":{"id":"Sf4OiMCAp6-X"},"source":["O parâmetro strip_handles, quando definido como True, remove as alças/arrobas mencionadas em um post / tweet. Como pode ser visto na saída anterior, @amankedia é removido, pois é um identificador.\n","Mais um parâmetro que está disponível com TweetTokenizer é preserve_case, que, quando definido como False, converte tudo para minúsculas a fim de normalizar o vocabulário. O valor padrão para este parâmetro é True."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Understanding Tokenization.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}